{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07_Ensemble_Learning_and_Random_Forests.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPDVnDqOMhU/9+8l5LibITe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carloslme/handson-ml2-book/blob/main/07_Ensemble_Learning_and_Random_Forests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swALwbfbxVmb"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMCGlSzexJZo"
      },
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"ensembles\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6n1q9H-xl-U"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "If you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than wiht the best individual predictor. \n",
        "\n",
        "A group of predictors is called *ensemble*, thus, this technique is called *Ensemble Learning*, and an Ensemble Learning algorithm is called an *Ensemble method*.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwAOrmE_2fXl"
      },
      "source": [
        "# Voting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyK8e-4sxcmu"
      },
      "source": [
        "heads_proba = 0.51\n",
        "coin_tosses = (np.random.rand(10000, 10) < heads_proba).astype(np.int32)\n",
        "cumulative_heads_ratio = np.cumsum(coin_tosses, axis=0) / np.arange(1, 10001).reshape(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpbZCO1m2g6D"
      },
      "source": [
        "plt.figure(figsize=(8,3.5))\n",
        "plt.plot(cumulative_heads_ratio)\n",
        "plt.plot([0, 10000], [0.51, 0.51], \"k--\", linewidth=2, label=\"51%\")\n",
        "plt.plot([0, 10000], [0.5, 0.5], \"k-\", label=\"50%\")\n",
        "plt.xlabel(\"Number of coin tosses\")\n",
        "plt.ylabel(\"Heads ratio\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.axis([0, 10000, 0.42, 0.58])\n",
        "save_fig(\"law_of_large_numbers_plot\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "perRJasV2mvq"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbKh3w0M28Y2"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
        "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "svm_clf = SVC(gamma=\"scale\", random_state=42)\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
        "    voting='hard')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZlgIjBs3BPQ"
      },
      "source": [
        "voting_clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A8FAl9A3DBe"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iknfaBbF3vFd"
      },
      "source": [
        "Ensemble method can be a *strong learner*, provided there are a sufficient number of weak learners and they are sufficiently diverse.\n",
        "\n",
        "Suppose you build an ensemble containing 1,000 classifiers that are individually correct only 50% of the time (barely better than random guessing). If you predict the majority voted class, hope for up to 75% accuracy. \n",
        "\n",
        "However, *this in only true if all classifiers are perfectly independent, making uncorrelated errors*, which is clearly not the cas because they are trained on the same data. They are likely to make the same types of errors, so there will be many majority votes for the wrong class, reducing the ensemble's accuracy.\n",
        "\n",
        "**TIP**\n",
        "\n",
        "Ensemble methods work best when the predictors are as independent from one another as possible. One way to get diverse classifiers is to train them using very different algorithms. This increases the chance that they will make very different types of errors, improving the ensemble’s accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eueyr9Ag6dwA"
      },
      "source": [
        "**Soft voting**\n",
        "\n",
        "If all classifiers are able to estimate class probabilities (i.e., they all have a `predict_proba()` method), then you can tell Scikit-Learn to predict the class with the highest class probability, averaged over all the individual classifiers. This is called *soft voting*. It often achieves higher performance that hard voting because it gives more weight to highly confident votes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W64CBk_R3wWk"
      },
      "source": [
        "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
        "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "svm_clf = SVC(gamma=\"scale\", probability=True, random_state=42) # probability=True and SVC will use CV to estimate probs. and adding a predict_proba() method.\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
        "    voting='soft')\n",
        "voting_clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZb7C0137X15"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MuqAdbg7yf5"
      },
      "source": [
        "Once the code modified whit *soft voting*, the voting classifier achieves 92% accuracy versus 91.2% of *hard voting*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uViParh8Gxj"
      },
      "source": [
        "#Bagging and Pasting\n",
        "\n",
        "When sampling is performed *with* replacement, this method is called *bagging* (short for bootstrap aggregating). When sampling is performed *without* replacement, it is called *pasting*.\n",
        "\n",
        "\n",
        "In other words, both bagging and pasting allow training instances to be sampled several times across multiple predictors, but only bagging allows training instances to be sampled several times for the same predictor.\n",
        "\n",
        "\n",
        "Once all predictors are trained, the ensemble can make a prediction for a new instance by simply aggregating the predictions of all predictors. The aggregation function is typically the *statistical mode* (i.e., the most frequent prediction, just like a hard voting classifier) for classification, or the average for regression.\n",
        "\n",
        "\n",
        "Predictors can all be trained in parallel, via different cores or even different servers. Similarly, predictions can be made in parallel. Similarly, predictions can be made in parallel. This is one of the reasons bagging and pasting are such popular methods: they scale very well.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcxMrMNpJaO0"
      },
      "source": [
        "Training 500 classifiers, each trained on 100 training instances randomly sampled from the training set with replacement "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNtwWLkN7iJh"
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(), n_estimators=500,\n",
        "    max_samples=100, bootstrap=True, random_state=42) # bootstrap=True for bagging, False for pasting\n",
        "bag_clf.fit(X_train, y_train)\n",
        "y_pred = bag_clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrtgEVUQGgVe"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLo0SW8LJ8Xc"
      },
      "source": [
        "**Note**\n",
        "\n",
        "The `BaggingClassifier` automatically performs soft voting isntrad of hard voting if the base classifier can estimate class probabilities (if it has `predict_proba()` method)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtnutGeLHtuG"
      },
      "source": [
        "tree_clf = DecisionTreeClassifier(random_state=42)\n",
        "tree_clf.fit(X_train, y_train)\n",
        "y_pred_tree = tree_clf.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred_tree))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZz57r62IPUJ"
      },
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "def plot_decision_boundary(clf, X, y, axes=[-1.5, 2.45, -1, 1.5], alpha=0.5, contour=True):\n",
        "    x1s = np.linspace(axes[0], axes[1], 100)\n",
        "    x2s = np.linspace(axes[2], axes[3], 100)\n",
        "    x1, x2 = np.meshgrid(x1s, x2s)\n",
        "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
        "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
        "    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
        "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
        "    if contour:\n",
        "        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
        "        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
        "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", alpha=alpha)\n",
        "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", alpha=alpha)\n",
        "    plt.axis(axes)\n",
        "    plt.xlabel(r\"$x_1$\", fontsize=18)\n",
        "    plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX2msKWnJNIs"
      },
      "source": [
        "\n",
        "fix, axes = plt.subplots(ncols=2, figsize=(10,4), sharey=True)\n",
        "plt.sca(axes[0])\n",
        "plot_decision_boundary(tree_clf, X, y)\n",
        "plt.title(\"Decision Tree\", fontsize=14)\n",
        "plt.sca(axes[1])\n",
        "plot_decision_boundary(bag_clf, X, y)\n",
        "plt.title(\"Decision Trees with Bagging\", fontsize=14)\n",
        "plt.ylabel(\"\")\n",
        "save_fig(\"decision_tree_without_and_with_bagging_plot\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZH3uN0ZYKUmB"
      },
      "source": [
        "In the figure above, it compares the decision boundary of a single Decision Tree with the decision boundary of a bagging ensemble of 500 trees, both trained on the moons dataset. \n",
        "\n",
        "THe ensemble's predictions will likely generalize much better than the single Decision Tree's predictions; the ensemble has a comparable bias but a smaller variance.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCatCeJQL0se"
      },
      "source": [
        "Bootstrapping introduces a bit more diversity in the subsets that each predictor is trained on, so bagging ends up with a slightly higher bias than pasting; but the extra diversity also means that the predictors end up being less correlated, so the ensemble's variance is reduced. **Overall, bagging often result in better models, which explains why it is generally preferred.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A7EhonCJoi1"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cM5j3KUKJO35"
      },
      "source": [
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(max_features=\"sqrt\", max_leaf_nodes=16),\n",
        "    n_estimators=500, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHdOgUg4Jq_A"
      },
      "source": [
        "\n",
        "bag_clf.fit(X_train, y_train)\n",
        "y_pred = bag_clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b3voXxmJsRB"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, random_state=42)\n",
        "rnd_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = rnd_clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjWijafVS2iM"
      },
      "source": [
        "np.sum(y_pred == y_pred_rf) / len(y_pred)  # very similar predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE3dDPJMS95i"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500, random_state=42)\n",
        "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
        "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
        "    print(name, score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_Q43DvPTCLf"
      },
      "source": [
        "rnd_clf.feature_importances_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wnl1vduTP9O"
      },
      "source": [
        "plt.figure(figsize=(6, 4))\n",
        "\n",
        "for i in range(15):\n",
        "    tree_clf = DecisionTreeClassifier(max_leaf_nodes=16, random_state=42 + i)\n",
        "    indices_with_replacement = np.random.randint(0, len(X_train), len(X_train))\n",
        "    tree_clf.fit(X[indices_with_replacement], y[indices_with_replacement])\n",
        "    plot_decision_boundary(tree_clf, X, y, axes=[-1.5, 2.45, -1, 1.5], alpha=0.02, contour=False)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtS4DMy4T73o"
      },
      "source": [
        "# Out-of-Bag evaluation\n",
        "\n",
        "With bagging, some instances may be sampled several times for any given predictor, while others may not be sampled at all. \n",
        "\n",
        "By default a `BaggingClassifier` samples *m* training instances with replacement ( `bootstrap=True` ), where *m* is the size of the training set. This means that only about 63% of the training instances are sampled on average for each predictor. The remaining 37% of the training instances that are not sampled are called out-of-bag (oob) instances. Note that they are not the same 37% for all predictors.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24adxVAtTR68"
      },
      "source": [
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(), n_estimators=500,\n",
        "    bootstrap=True, oob_score=True, random_state=40)\n",
        "bag_clf.fit(X_train, y_train)\n",
        "bag_clf.oob_score_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_pzlYgWT_Ue"
      },
      "source": [
        "bag_clf.oob_decision_function_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHdx8lv8UBcr"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = bag_clf.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quhsTDndNNAz"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nXo7MRsYzyu"
      },
      "source": [
        "# Feature importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1F4ditXhVro"
      },
      "source": [
        "Another great quality of Random Forests is that they make it easy to measure the relative importance of each feature. Scikit-Learn measures a feature’s importance by looking at how much the tree nodes that use that feature reduce impurity on average (across all trees in the forest).\n",
        "\n",
        "More precisely, it is a weighted average, where each node’s weight is equal to the number of training samples that are associated with it. Scikit-Learn computes this score automatically for each feature after training, then it scales the results so that the sum of all importances is equal to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s47Y1K00UPKH"
      },
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
        "mnist.target = mnist.target.astype(np.uint8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DPdens0Y_Ky"
      },
      "source": [
        "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rnd_clf.fit(mnist[\"data\"], mnist[\"target\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHpBLUJOZWhp"
      },
      "source": [
        "def plot_digit(data):\n",
        "    image = data.reshape(28, 28)\n",
        "    plt.imshow(image, cmap = mpl.cm.hot,\n",
        "               interpolation=\"nearest\")\n",
        "    plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKJF1Kqmajoe"
      },
      "source": [
        "plot_digit(rnd_clf.feature_importances_)\n",
        "\n",
        "cbar = plt.colorbar(ticks=[rnd_clf.feature_importances_.min(), rnd_clf.feature_importances_.max()])\n",
        "cbar.ax.set_yticklabels(['Not important', 'Very important'])\n",
        "\n",
        "save_fig(\"mnist_feature_importance_plot\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-SOJjw0iHeO"
      },
      "source": [
        "# Boosting\n",
        "Called *hypothesys boosting*, refers to any Ensemble method that combine several weak learners into a strong learner. The general idea of moost boosting methods is to train predictors sequentally, each trying to correct its predecessor. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ksn9Jk1saxSX"
      },
      "source": [
        "## AdaBoost\n",
        "\n",
        "One way for a new predictor to correct its predecessor is to **pay a bit more attention to the training instances that the predecessor underfitted.** This results in new predictors focusing more and more on the hard cases. This is the technique used by AdaBoost.\n",
        "\n",
        "When training an AdaBoost classifier, the algorithm first trains a base classifier (such as a Decision Tree) and uses it to make predictions on the training set. The algorithm then increases the relative weight of misclassified training instances. Then it trains a second classifier, using the updated weights, and again makes predictions on the training set, updates the instance weights, and so on. The sequential learning technique has some similarities with Gradient Descent, except that instead of tweaking a single predictor’s parameters to minimize a cost function, **AdaBoost adds predictors to the ensemble, gradually making it better.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmFBvLTUaklX"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "ada_clf = AdaBoostClassifier(\n",
        "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
        "    algorithm=\"SAMME.R\", learning_rate=0.5, random_state=42)\n",
        "ada_clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxYA-hdebgXI"
      },
      "source": [
        "m = len(X_train)\n",
        "\n",
        "fix, axes = plt.subplots(ncols=2, figsize=(10,4), sharey=True)\n",
        "for subplot, learning_rate in ((0, 1), (1, 0.5)):\n",
        "    sample_weights = np.ones(m) / m\n",
        "    plt.sca(axes[subplot])\n",
        "    for i in range(5):\n",
        "        svm_clf = SVC(kernel=\"rbf\", C=0.2, gamma=0.6, random_state=42)\n",
        "        svm_clf.fit(X_train, y_train, sample_weight=sample_weights * m)\n",
        "        y_pred = svm_clf.predict(X_train)\n",
        "\n",
        "        r = sample_weights[y_pred != y_train].sum() / sample_weights.sum() # equation 7-1\n",
        "        alpha = learning_rate * np.log((1 - r) / r) # equation 7-2\n",
        "        sample_weights[y_pred != y_train] *= np.exp(alpha) # equation 7-3\n",
        "        sample_weights /= sample_weights.sum() # normalization step\n",
        "\n",
        "        plot_decision_boundary(svm_clf, X, y, alpha=0.2)\n",
        "        plt.title(\"learning_rate = {}\".format(learning_rate), fontsize=16)\n",
        "    if subplot == 0:\n",
        "        plt.text(-0.75, -0.95, \"1\", fontsize=14)\n",
        "        plt.text(-1.05, -0.95, \"2\", fontsize=14)\n",
        "        plt.text(1.0, -0.95, \"3\", fontsize=14)\n",
        "        plt.text(-1.45, -0.5, \"4\", fontsize=14)\n",
        "        plt.text(1.36,  -0.95, \"5\", fontsize=14)\n",
        "    else:\n",
        "        plt.ylabel(\"\")\n",
        "\n",
        "save_fig(\"boosting_plot\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCMBuN3cnIeI"
      },
      "source": [
        "**Warning**\n",
        "\n",
        "The drawback to this sequential learning technique is that it cannot be parallelized (or only partially), since each predictor can only be trained after the previous has been trained an evaluated. As a result, it does not scale as well as bagging or pasting.\n",
        "\n",
        "**Tip**\n",
        "\n",
        "\n",
        "If your AdaBoost ensemble is overfitting the training set, you can try reducing the number of estimators or more strongly regularizing the base estimator.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3O8BESvoT1m"
      },
      "source": [
        "## Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YW9JETGDdOB5"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) - 0.5\n",
        "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKqpfY9xojP4"
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
        "tree_reg1.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xeyw6FJopll"
      },
      "source": [
        "y2 = y - tree_reg1.predict(X)\n",
        "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
        "tree_reg2.fit(X, y2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJYMSKqjvtej"
      },
      "source": [
        "y3 = y2 - tree_reg2.predict(X)\n",
        "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
        "tree_reg3.fit(X, y3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2Xawg2Uv4tp"
      },
      "source": [
        "X_new = np.array([[0.8]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWwnuCe-v_oK"
      },
      "source": [
        "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xmWprw1wE9I"
      },
      "source": [
        "y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MteTStKswG6v"
      },
      "source": [
        "def plot_predictions(regressors, X, y, axes, label=None, style=\"r-\", data_style=\"b.\", data_label=None):\n",
        "    x1 = np.linspace(axes[0], axes[1], 500)\n",
        "    y_pred = sum(regressor.predict(x1.reshape(-1, 1)) for regressor in regressors)\n",
        "    plt.plot(X[:, 0], y, data_style, label=data_label)\n",
        "    plt.plot(x1, y_pred, style, linewidth=2, label=label)\n",
        "    if label or data_label:\n",
        "        plt.legend(loc=\"upper center\", fontsize=16)\n",
        "    plt.axis(axes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23hz69e5wJW2"
      },
      "source": [
        "plt.figure(figsize=(11,11))\n",
        "\n",
        "plt.subplot(321)\n",
        "plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h_1(x_1)$\", style=\"g-\", data_label=\"Training set\")\n",
        "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
        "plt.title(\"Residuals and tree predictions\", fontsize=16)\n",
        "\n",
        "plt.subplot(322)\n",
        "plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1)$\", data_label=\"Training set\")\n",
        "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
        "plt.title(\"Ensemble predictions\", fontsize=16)\n",
        "\n",
        "plt.subplot(323)\n",
        "plot_predictions([tree_reg2], X, y2, axes=[-0.5, 0.5, -0.5, 0.5], label=\"$h_2(x_1)$\", style=\"g-\", data_style=\"k+\", data_label=\"Residuals\")\n",
        "plt.ylabel(\"$y - h_1(x_1)$\", fontsize=16)\n",
        "\n",
        "plt.subplot(324)\n",
        "plot_predictions([tree_reg1, tree_reg2], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1) + h_2(x_1)$\")\n",
        "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
        "\n",
        "plt.subplot(325)\n",
        "plot_predictions([tree_reg3], X, y3, axes=[-0.5, 0.5, -0.5, 0.5], label=\"$h_3(x_1)$\", style=\"g-\", data_style=\"k+\")\n",
        "plt.ylabel(\"$y - h_1(x_1) - h_2(x_1)$\", fontsize=16)\n",
        "plt.xlabel(\"$x_1$\", fontsize=16)\n",
        "\n",
        "plt.subplot(326)\n",
        "plot_predictions([tree_reg1, tree_reg2, tree_reg3], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1) + h_2(x_1) + h_3(x_1)$\")\n",
        "plt.xlabel(\"$x_1$\", fontsize=16)\n",
        "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
        "\n",
        "save_fig(\"gradient_boosting_plot\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1hnxcCLwLvv"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=42)\n",
        "gbrt.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zW8XuPIL6Xpn"
      },
      "source": [
        "gbrt_slow = GradientBoostingRegressor(max_depth=2, n_estimators=200, learning_rate=0.1, random_state=42)\n",
        "gbrt_slow.fit(X, y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lkg8aTQ66nb5"
      },
      "source": [
        "fix, axes = plt.subplots(ncols=2, figsize=(10,4), sharey=True)\n",
        "\n",
        "plt.sca(axes[0])\n",
        "plot_predictions([gbrt], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"Ensemble predictions\")\n",
        "plt.title(\"learning_rate={}, n_estimators={}\".format(gbrt.learning_rate, gbrt.n_estimators), fontsize=14)\n",
        "plt.xlabel(\"$x_1$\", fontsize=16)\n",
        "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
        "\n",
        "plt.sca(axes[1])\n",
        "plot_predictions([gbrt_slow], X, y, axes=[-0.5, 0.5, -0.1, 0.8])\n",
        "plt.title(\"learning_rate={}, n_estimators={}\".format(gbrt_slow.learning_rate, gbrt_slow.n_estimators), fontsize=14)\n",
        "plt.xlabel(\"$x_1$\", fontsize=16)\n",
        "\n",
        "save_fig(\"gbrt_learning_rate_plot\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpOYqSYg677F"
      },
      "source": [
        "# Gradient Boosting with Early stopping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ozokCRE6pw-"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=49)\n",
        "\n",
        "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120, random_state=42)\n",
        "gbrt.fit(X_train, y_train)\n",
        "\n",
        "errors = [mean_squared_error(y_val, y_pred)\n",
        "          for y_pred in gbrt.staged_predict(X_val)]\n",
        "bst_n_estimators = np.argmin(errors) + 1\n",
        "\n",
        "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators, random_state=42)\n",
        "gbrt_best.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoBvaKSM7CIb"
      },
      "source": [
        "GradientBoostingRegressor(max_depth=2, n_estimators=56, random_state=42)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEBYug_87ERJ"
      },
      "source": [
        "min_error = np.min(errors)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er-FSE1d7FyW"
      },
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.plot(errors, \"b.-\")\n",
        "plt.plot([bst_n_estimators, bst_n_estimators], [0, min_error], \"k--\")\n",
        "plt.plot([0, 120], [min_error, min_error], \"k--\")\n",
        "plt.plot(bst_n_estimators, min_error, \"ko\")\n",
        "plt.text(bst_n_estimators, min_error*1.2, \"Minimum\", ha=\"center\", fontsize=14)\n",
        "plt.axis([0, 120, 0, 0.01])\n",
        "plt.xlabel(\"Number of trees\")\n",
        "plt.ylabel(\"Error\", fontsize=16)\n",
        "plt.title(\"Validation error\", fontsize=14)\n",
        "\n",
        "plt.subplot(122)\n",
        "plot_predictions([gbrt_best], X, y, axes=[-0.5, 0.5, -0.1, 0.8])\n",
        "plt.title(\"Best model (%d trees)\" % bst_n_estimators, fontsize=14)\n",
        "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
        "plt.xlabel(\"$x_1$\", fontsize=16)\n",
        "\n",
        "save_fig(\"early_stopping_gbrt_plot\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvEmECiT7H4A"
      },
      "source": [
        "\n",
        "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True, random_state=42)\n",
        "\n",
        "min_val_error = float(\"inf\")\n",
        "error_going_up = 0\n",
        "for n_estimators in range(1, 120):\n",
        "    gbrt.n_estimators = n_estimators\n",
        "    gbrt.fit(X_train, y_train)\n",
        "    y_pred = gbrt.predict(X_val)\n",
        "    val_error = mean_squared_error(y_val, y_pred)\n",
        "    if val_error < min_val_error:\n",
        "        min_val_error = val_error\n",
        "        error_going_up = 0\n",
        "    else:\n",
        "        error_going_up += 1\n",
        "        if error_going_up == 5:\n",
        "            break  # early stopping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2yIvD4Q_KWZ"
      },
      "source": [
        "print(gbrt.n_estimators)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zu6VImL_OoD"
      },
      "source": [
        "print(\"Minimum validation MSE:\", min_val_error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z-puLF2_1Sb"
      },
      "source": [
        "# Using XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27ANuf0U_Qie"
      },
      "source": [
        "try:\n",
        "    import xgboost\n",
        "except ImportError as ex:\n",
        "    print(\"Error: the xgboost library is not installed.\")\n",
        "    xgboost = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxQ-wCnsAHDI"
      },
      "source": [
        "if xgboost is not None:  # not shown in the book\n",
        "    xgb_reg = xgboost.XGBRegressor(random_state=42)\n",
        "    xgb_reg.fit(X_train, y_train)\n",
        "    y_pred = xgb_reg.predict(X_val)\n",
        "    val_error = mean_squared_error(y_val, y_pred) # Not shown\n",
        "    print(\"Validation MSE:\", val_error)           # Not shown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFta5eCiALbO"
      },
      "source": [
        "if xgboost is not None:  # not shown in the book\n",
        "    xgb_reg.fit(X_train, y_train,\n",
        "                eval_set=[(X_val, y_val)], early_stopping_rounds=2)\n",
        "    y_pred = xgb_reg.predict(X_val)\n",
        "    val_error = mean_squared_error(y_val, y_pred)  # Not shown\n",
        "    print(\"Validation MSE:\", val_error)            # Not shown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VzlIkcFAz-U"
      },
      "source": [
        "%timeit xgboost.XGBRegressor().fit(X_train, y_train) if xgboost is not None else None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrRjJ1ASBPxf"
      },
      "source": [
        "%timeit GradientBoostingRegressor().fit(X_train, y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4taq5mLNBVM-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0uu9hxzOmol"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-K0qzLKOpVm"
      },
      "source": [
        "## Voting classifiers\n",
        "Exercise: Load the MNIST data and split it into a training set, a validation set, and a test set (e.g., use 50,000 instances for training, 10,000 for validation, and 10,000 for testing).\n",
        "\n",
        "The MNIST dataset was loaded earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IUdlWwxOosg"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtK5v3lTPPxQ"
      },
      "source": [
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    mnist.data, mnist.target, test_size=10000, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=10000, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgHRr5O7PYd3"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neural_network import MLPClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKAeFEjPPeLD"
      },
      "source": [
        "random_forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "extra_trees_clf = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
        "svm_clf = LinearSVC(max_iter=100, tol=20, random_state=42)\n",
        "mlp_clf = MLPClassifier(random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}